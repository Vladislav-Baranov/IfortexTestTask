1. Выбор модели
Среди предложенных моделей я решил выбрать bart-large-cnn, так как она обучена на новостном датасете и служит специально для суммаризации. Модели t5-small / t5-base хуже подходят для наших целей, а mistral-7b-instruct слишком требовательна к ресурсам.

2. Загрузка текста в программу
Реализовано три способа загрузки теста: вставить текст, загрузить из файла, загрузить по ссылке 

3. Работа с большим текстом
Основной задачей в ходе работы на мой взгляд являлась задача обработки длинного текста. Для её решения я воспользовался механизмом скользящего окна. Такой подход часто используется в архитектурах, генерирующих ответ на основе предыдущего контекста. Разбивка по абзацам может привести к потере смысловой нити, а также сама по себе необъективна: в тексте разделение может быть выполнено неверно, из-за чего повествование получится рваным. Итеративная генерация и финальное сжатие -- не самые простые способы на мой взгляд. В итоге по критерию цена/качество победило скользящее окно. 

